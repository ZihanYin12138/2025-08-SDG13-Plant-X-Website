{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Plant Species Details — Data Pull Notebook\n",
    "\n",
    "Name: Zihan Yin\n",
    "\n",
    "This notebook fetches plant species detail JSON from Perenual’s Open API and saves one file per species ID. I kept it simple: one API key, polite rate-limiting, retry logic, and “safe stop” when I hit daily quotas. Run top-to-bottom; it’s restart-friendly and will skip files that already exist.    \n",
    "\n",
    "Before each time running, teammates can fetch from our github reporsitory first, then go through step 1 - 4, which means simply click \"Run All\" is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Configuration (range, output paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set my API key, the ID range to pull, some conservative rate-limit settings, and where to save the files. Paths use `/` so they work cross-platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: ****112084, Range: 1~3000, Output dir: E:\\05_YZH_DS\\02_Monash_DS\\2025_S2_FIT5120_Industry_Experience_Studio_Project\\06_main_project\\03_github_submission\\03_github_submission\\2025-08-SDG13-Plant-X-Website\\01_data_wrangling\\01_raw_data\\01_species_details\n"
     ]
    }
   ],
   "source": [
    "# I put my API key here. Replace with your own.\n",
    "API_KEY = \"sk-kMpx68b02586bb61112084\"\n",
    "\n",
    "# IDs I want to fetch (I usually test with a small range first).\n",
    "START_ID = 1\n",
    "END_ID   = 3000\n",
    "\n",
    "# Rate limit & retry settings \n",
    "SLEEP_BETWEEN = 1        # seconds to sleep after each request\n",
    "MAX_RETRIES   = 5\n",
    "BACKOFF_BASE  = 1.6\n",
    "\n",
    "# Output directory & filename pattern.\n",
    "# I use forward slashes; pathlib will adapt automatically on Windows/macOS/Linux.\n",
    "from pathlib import Path\n",
    "OUT_DIR = Path(\"01_raw_data/01_species_details\")\n",
    "FILENAME_PATTERN = \"plant_species_details_{species_id}.json\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Key: ****{API_KEY[-6:]}, Range: {START_ID}~{END_ID}, Output dir: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Some Small Helper Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tiny save function and a path builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "import requests\n",
    "\n",
    "def save_json(path: Path, data: Dict[str, Any]):\n",
    "    # UTF-8 so everything is readable.\n",
    "    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def build_filepath(species_id: int) -> Path:\n",
    "    # One JSON per species_id, based on the pattern above.\n",
    "    return OUT_DIR / FILENAME_PATTERN.format(species_id=species_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Fetch one species (with retries & safe-stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual fetcher. It handles 404s (I save a placeholder), retries on 429/5xx with exponential backoff, and “safe-stops” if it keep hitting 429 or the response headers say my daily quota is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_species_details(species_id: int, api_key: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Success -> returns a JSON dict\n",
    "    404     -> returns {\"__missing__\": True, \"id\": species_id} as a placeholder\n",
    "    None    -> signals the main loop to stop (e.g., quota/rate limit reached)\n",
    "    \"\"\"\n",
    "    url = f\"https://perenual.com/api/v2/species/details/{species_id}\"\n",
    "    attempt = 0\n",
    "    consecutive_429 = 0\n",
    "\n",
    "    while attempt <= MAX_RETRIES:\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                url,\n",
    "                params={\"key\": api_key},\n",
    "                headers={\"accept\": \"application/json\"},\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            # The API will tells me the remaining quota\n",
    "            remaining = resp.headers.get(\"x-ratelimit-remaining\")\n",
    "            if remaining is not None:\n",
    "                try:\n",
    "                    if int(remaining) <= 0:\n",
    "                        print(f\"[ID {species_id}] x-ratelimit-remaining=0 → I’ll stop safely (quota reached).\")\n",
    "                        return None\n",
    "                except ValueError:\n",
    "                    pass  # if it's not an integer, just ignore it\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                return resp.json()\n",
    "\n",
    "            if resp.status_code == 404:\n",
    "                print(f\"[ID {species_id}] 404: not found / no data. I’ll save a placeholder.\")\n",
    "                return {\"__missing__\": True, \"id\": species_id}\n",
    "\n",
    "            if resp.status_code == 429:\n",
    "                consecutive_429 += 1\n",
    "                # If hit too many 429s in a row, it’s likely the daily/IP limit.\n",
    "                if consecutive_429 >= 3:\n",
    "                    print(f\"[ID {species_id}] 429 happened {consecutive_429} times in a row → I’ll stop safely.\")\n",
    "                    return None\n",
    "                wait = (BACKOFF_BASE ** attempt) + 0.2 * attempt\n",
    "                print(f\"[ID {species_id}] 429 Too Many Requests, waiting {wait:.1f}s before retrying…\")\n",
    "                time.sleep(wait)\n",
    "                attempt += 1\n",
    "                continue\n",
    "\n",
    "            if resp.status_code in (500, 502, 503, 504):\n",
    "                wait = (BACKOFF_BASE ** attempt) + 0.2 * attempt\n",
    "                print(f\"[ID {species_id}] {resp.status_code} server error, waiting {wait:.1f}s then retrying…\")\n",
    "                time.sleep(wait)\n",
    "                attempt += 1\n",
    "                continue\n",
    "\n",
    "            # For other unexpected status codes, I log a short snippet for context.\n",
    "            print(f\"[ID {species_id}] HTTP {resp.status_code}: {resp.text[:200]}\")\n",
    "            return {\"__error_status__\": resp.status_code, \"id\": species_id}\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            wait = (BACKOFF_BASE ** attempt) + 0.2 * attempt\n",
    "            print(f\"[ID {species_id}] Network error ({type(e).__name__}): {e}. Waiting {wait:.1f}s then retrying…\")\n",
    "            time.sleep(wait)\n",
    "            attempt += 1\n",
    "\n",
    "    print(f\"[ID {species_id}] Retries exhausted. I’m giving up on this one.\")\n",
    "    return {\"__error_retries_exhausted__\": True, \"id\": species_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Main loop (resume-friendly, skips existing files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I iterate over the ID range, skip files that already exist, and pause a bit between calls. If the fetcher returns None, I assume quota/rate limit and stop politely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zyyin1\\anaconda3\\envs\\python_for_data_analysis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading Species Details:  49%|████▉     | 1484/3000 [03:45<03:50,  6.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ID 1485] x-ratelimit-remaining=0 → I’ll stop safely (quota reached).\n",
      "Quota/rate-limit inferred → stopping safely.\n",
      "New downloads: 99, skipped (already exists): 1385. Output dir: E:\\05_YZH_DS\\02_Monash_DS\\2025_S2_FIT5120_Industry_Experience_Studio_Project\\06_main_project\\03_github_submission\\03_github_submission\\2025-08-SDG13-Plant-X-Website\\01_data_wrangling\\01_raw_data\\01_species_details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "downloaded = 0\n",
    "skipped = 0\n",
    "\n",
    "for species_id in tqdm(range(START_ID, END_ID + 1), desc=\"Downloading Species Details\"):\n",
    "    fp = build_filepath(species_id)\n",
    "    if fp.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    data = fetch_species_details(species_id, API_KEY)\n",
    "    if data is None:\n",
    "        print(\"Quota/rate-limit inferred → stopping safely.\")\n",
    "        break\n",
    "\n",
    "    save_json(fp, data)\n",
    "    downloaded += 1\n",
    "    time.sleep(SLEEP_BETWEEN + random.uniform(0.0, 0.6))\n",
    "\n",
    "print(f\"New downloads: {downloaded}, skipped (already exists): {skipped}. Output dir: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (Optional) - Pack and download as a zip (for Colab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I’m on Colab, this zips the folder and downloads it to my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from google.colab import files\n",
    "#\n",
    "# # Zip the entire folder\n",
    "# shutil.make_archive(\"01_species_details\", 'zip', \"01_raw_data/01_species_details\")\n",
    "#\n",
    "# # Download to my local machine\n",
    "# files.download(\"01_species_details.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

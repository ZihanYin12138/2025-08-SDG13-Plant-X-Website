{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threatened Species Index: Calculation and Database Import\n",
    "\n",
    "Name: Zihan\n",
    "\n",
    "The following steps cover the end-to-end process of calculating the Threatened Species Index from raw data, creating a dedicated table in the MySQL database, and importing the final results for storage and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow (Simplified GAM Method)\n",
    "\n",
    "1.  **Read Raw Data**: Load the initial dataset from the source CSV file.\n",
    "2.  **Select Columns**: Filter the data to include only the year columns from 1950 to 2020.\n",
    "3.  **Process Each Species**: For every species in the dataset, perform the following steps:\n",
    "    * **Extract Time Series**: Isolate the time series for the species. Exclude any series that is entirely empty or is missing data for the designated reference year.\n",
    "    * **Normalize Data**: Set 1985 as the reference year. Standardize the time series by dividing all values by the value in 1985, making the reference year's value equal to `1`.\n",
    "    * **Fit GAM Model**: Apply a Generalized Additive Model (using the `pyGAM` library) to fit a smoothed trend to the normalized time series.\n",
    "    * **Predict Trend**: Use the fitted GAM to predict the trend value for each year from 1950 to 2020.\n",
    "    * **Calculate Species-Level Index**: The index for the species is the series of predicted values divided by the predicted value of the reference year (1985).\n",
    "4.  **Aggregate to Global Level**:\n",
    "    * Combine the indices from all species by calculating the average value for each year.\n",
    "    * Generate bootstrap confidence intervals (`low`, `high`) for the annual averages.\n",
    "5.  **Export Results**: Save the final aggregated data with the columns: `year`, `value`, `low`, `high`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow (Simplified GAM Method) (Chinese Version)\n",
    "\n",
    "1.  **Read Raw Data**: Load the initial dataset from the source CSV file.\n",
    "2.  **Select Data Columns**: Filter the data to include only year columns from 1950 to 2020.\n",
    "3.  **Process Each Species**: For every species in the dataset, perform the following steps:\n",
    "    * **Extract Time Series**: Isolate the time series data for the species. Exclude any series that is entirely empty or missing data for the reference year (1985).\n",
    "    * **Normalize Data**: Set 1985 as the reference year. Standardize the time series by dividing all values by the value in 1985, making the reference year's value equal to `1`.\n",
    "    * **Fit GAM Model**: Apply a Generalized Additive Model (using the `pyGAM` library) to fit a smoothed trend to the normalized time series.\n",
    "    * **Predict Trend**: Use the fitted GAM model to predict trend values for each year from 1950 to 2020.\n",
    "    * **Calculate Species-Level Index**: The index for the species is the series of predicted values divided by the predicted value of the reference year (1985).\n",
    "4.  **Aggregate to Global Level**:\n",
    "    * Combine the indices from all species by calculating the average value for each year.\n",
    "    * Generate bootstrap confidence intervals (`low`, `high`) for the annual averages.\n",
    "5.  **Export Results**: Save the final aggregated data with the columns: `year`, `value`, `low`, `high`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Calculate Threatened Species Index using GAM and Bootstrapping\n",
    "\n",
    "This script calculates a composite macro-level index from a large number of individual species' time-series data. It employs a Generalized Additive Model (GAM) to smooth the trend for each species and uses bootstrapping to estimate the confidence interval of the overall trend.\n",
    "\n",
    "The workflow can be broken down into the following key steps:\n",
    "\n",
    "1.  **Initialization and Data Loading**: It starts by setting essential parameters, such as the reference year (`1985`), whether to apply a time lag, and bootstrap settings. It then loads the raw time-series data from the CSV file.\n",
    "2.  **Per-Species Processing**:\n",
    "    * **Normalization**: For each species' time series, the data is standardized against the value in the reference year, effectively setting the index for `1985 = 1`.\n",
    "    * **Smoothing and Fitting**: A GAM is fitted to the normalized data. This generates a smooth trend curve, which helps to fill in data gaps and reduce year-to-year noise.\n",
    "3.  **Aggregation and Statistical Analysis**:\n",
    "    * **Calculate Mean Trend**: All the individual smoothed curves are aggregated by taking the average across all species for each year. This results in the main index trend (`value`).\n",
    "    * **Estimate Uncertainty**: To quantify the confidence in the mean trend, a bootstrapping method is applied. It randomly resamples the species thousands of times (`n_boot=1000`), recalculates the mean trend for each sample, and uses the resulting distribution to determine the 95% confidence interval (`low` and `high` values).\n",
    "4.  **Export Results**: The final calculated columns (`year`, `value`, `low`, `high`) are saved to a new CSV file, ready for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指数表已保存（基准年=1985，滞后=True）：02_wrangled_data\\Table14_ThreatenedSpeciesIndexTable_version2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s\n",
    "\n",
    "# ====== Paths ======\n",
    "input_path = r\"01_raw_data\\02_tsx-aggregated-data-dataset.csv\"\n",
    "output_path = r\"02_wrangled_data\\Table14_TSX_Table_VIC_version2.csv\"\n",
    "\n",
    "# ====== Parameters ======\n",
    "ref_year = 1985\n",
    "apply_lag = True          # Whether to apply 3-year lag\n",
    "lag_years = 3\n",
    "n_splines = 10\n",
    "n_boot = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ====== Read data ======\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Year columns\n",
    "year_cols = [c for c in df.columns if c.isdigit()]\n",
    "years = np.array(list(map(int, year_cols)))\n",
    "if ref_year not in years:\n",
    "    raise ValueError(f\"Reference year {ref_year} not found in data year columns!\")\n",
    "ref_idx = list(years).index(ref_year)\n",
    "\n",
    "# ====== Apply GAM fitting to each time series (first normalize with 1985) ======\n",
    "species_curves = []\n",
    "for _, row in df.iterrows():\n",
    "    y = row[year_cols].values.astype(float)\n",
    "\n",
    "    # Reference year must be valid and >0\n",
    "    if np.isnan(y[ref_idx]) or y[ref_idx] <= 0:\n",
    "        continue\n",
    "\n",
    "    # Normalize (set 1985 as 1)\n",
    "    y = y / y[ref_idx]\n",
    "\n",
    "    # Only use years with values for fitting\n",
    "    mask = ~np.isnan(y)\n",
    "    x_obs = years[mask]\n",
    "    y_obs = y[mask]\n",
    "\n",
    "    # Need at least 3 points to fit\n",
    "    if len(x_obs) < 3:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        gam = LinearGAM(s(0, n_splines=n_splines)).fit(x_obs, y_obs)\n",
    "        y_pred = gam.predict(years)  # Predict full time period\n",
    "        species_curves.append(y_pred)\n",
    "    except Exception:\n",
    "        # Some extreme sequences may fail to fit, just skip\n",
    "        continue\n",
    "\n",
    "if len(species_curves) == 0:\n",
    "    raise RuntimeError(\"No successfully fitted species curves, check reference year or data quality.\")\n",
    "\n",
    "species_curves = np.vstack(species_curves)  # Shape: [number of species, number of years]\n",
    "\n",
    "# ====== 只保留 >= 1985 的年份；可选再做“末尾-3年”滞后 ======\n",
    "mask_years = years >= ref_year\n",
    "years_out = years[mask_years]\n",
    "curves = species_curves[:, mask_years]\n",
    "\n",
    "if apply_lag:\n",
    "    # Remove the last lag_years years\n",
    "    if len(years_out) > lag_years:\n",
    "        years_out = years_out[:-lag_years]\n",
    "        curves = curves[:, :-lag_years]\n",
    "\n",
    "# ====== 用bootstrap对“物种曲线集合”聚合，得到 value/low/high ======\n",
    "# 先计算“物种横截面均值曲线”\n",
    "mean_curve = np.nanmean(curves, axis=0)\n",
    "\n",
    "# 再通过对物种维度重采样，得到均值的不确定性\n",
    "boot_mat = np.empty((n_boot, curves.shape[1]), dtype=float)\n",
    "n_species = curves.shape[0]\n",
    "for b in range(n_boot):\n",
    "    idx = rng.integers(0, n_species, size=n_species)\n",
    "    sample = curves[idx, :]\n",
    "    boot_mat[b, :] = np.nanmean(sample, axis=0)\n",
    "\n",
    "low_ci = np.nanpercentile(boot_mat, 2.5, axis=0)\n",
    "high_ci = np.nanpercentile(boot_mat, 97.5, axis=0)\n",
    "\n",
    "# ====== Force the three columns of reference year to be 1 (align with official style) ======\n",
    "# Note that the first element of years_out should be ref_year (since we truncated <1985)\n",
    "if years_out[0] == ref_year:\n",
    "    mean_curve[0] = 1.0\n",
    "    low_ci[0] = 1.0\n",
    "    high_ci[0] = 1.0\n",
    "\n",
    "# ====== Export ======\n",
    "result = pd.DataFrame({\n",
    "    \"year\": years_out,\n",
    "    \"value\": mean_curve,\n",
    "    \"low\": low_ci,\n",
    "    \"high\": high_ci\n",
    "})\n",
    "result.to_csv(output_path, index=False)\n",
    "print(f\"Index table saved (reference year={ref_year}, lag={apply_lag}): {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Create Schema for Threatened Species Index Table\n",
    "\n",
    "This step connects to the MySQL database and defines the structure for our new table, `Table14_ThreatenedSpeciesIndexTable`. The schema is designed to hold the time-series index data:\n",
    "- `year` is set as an `INT` and serves as the Primary Key.\n",
    "- `value`, `low`, and `high` are defined as `DOUBLE` to accommodate floating-point numbers.\n",
    "\n",
    "The `CREATE TABLE IF NOT EXISTS` command is used to ensure the script can be run multiple times without causing an error if the table already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL server, preparing to create Table14.\n",
      "Table 'Table14_TSX_Table_VIC' created successfully or already exists.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# --- Database connection configuration ---\n",
    "db_config = {\n",
    "    'host': 'database-plantx.cqz06uycysiz.us-east-1.rds.amazonaws.com',\n",
    "    'user': 'zihan',\n",
    "    'password': '2002317Yzh12138.',\n",
    "    'database': 'FIT5120_PlantX_Database',\n",
    "    'allow_local_infile': True,\n",
    "    'use_pure': True,\n",
    "    'charset': 'utf8mb4'\n",
    "}\n",
    "\n",
    "# --- Create the schema for Table14 ---\n",
    "try:\n",
    "    # Establish the database connection\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        print(\"Successfully connected to MySQL server, preparing to create Table14.\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Define the SQL query to create Table14\n",
    "        # `year` is set as an INT PRIMARY KEY\n",
    "        # `value`, `low`, and `high` are set as DOUBLE for floating-point numbers\n",
    "        create_table_14_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Table14_TSX_Table_VIC (\n",
    "            `year` INT PRIMARY KEY,\n",
    "            `value` DOUBLE,\n",
    "            `low` DOUBLE,\n",
    "            `high` DOUBLE\n",
    "        ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(create_table_14_query)\n",
    "        connection.commit()\n",
    "        print(\"Table 'Table14_TSX_Table_VIC' created successfully or already exists.\")\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error occurred while creating Table14: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Load CSV Data into Threatened Species Index Table\n",
    "\n",
    "After creating the table schema, this step populates it with data from the local CSV file (`02_wrangled_data/Table14_ThreatenedSpeciesIndexTable_version1.csv`).\n",
    "\n",
    "It uses the `LOAD DATA LOCAL INFILE` command, which is a highly efficient method for bulk-inserting data directly from a file into a MySQL table. The query is configured to handle the CSV format by specifying the field and line terminators and to skip the header row (`IGNORE 1 LINES`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL server, preparing to import data for Table14.\n",
      "Data import for Table14 successful! 37 rows affected.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# --- Import CSV data into Table14 ---\n",
    "try:\n",
    "    # Re-establish the database connection\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        print(\"Successfully connected to MySQL server, preparing to import data for Table14.\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Define the SQL query for loading data\n",
    "        # Using LOAD DATA LOCAL INFILE for efficient bulk import\n",
    "        load_data_query_14 = \"\"\"\n",
    "        LOAD DATA LOCAL INFILE '02_wrangled_data/Table14_TSX_Table_VIC_version1.csv'\n",
    "        INTO TABLE Table14_TSX_Table_VIC\n",
    "        CHARACTER SET utf8mb4\n",
    "        FIELDS TERMINATED BY ','\n",
    "        LINES TERMINATED BY '\\\\r\\\\n'\n",
    "        IGNORE 1 LINES\n",
    "        (year, value, low, high);\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(load_data_query_14)\n",
    "        connection.commit()\n",
    "        print(f\"Data import for Table14 successful! {cursor.rowcount} rows affected.\")\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error occurred during data import for Table14: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

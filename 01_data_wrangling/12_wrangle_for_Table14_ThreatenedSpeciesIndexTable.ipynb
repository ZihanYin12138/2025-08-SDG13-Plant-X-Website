{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threatened Species Index: Calculation and Database Import\n",
    "\n",
    "The following steps cover the end-to-end process of calculating the Threatened Species Index from raw data, creating a dedicated table in the MySQL database, and importing the final results for storage and use.\n",
    "\n",
    "Name: Zihan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow (Simplified GAM Method)\n",
    "\n",
    "1.  **Read Raw Data**: Load the initial dataset from the source CSV file.\n",
    "2.  **Select Columns**: Filter the data to include only the year columns from 1950 to 2020.\n",
    "3.  **Process Each Species**: For every species in the dataset, perform the following steps:\n",
    "    * **Extract Time Series**: Isolate the time series for the species. Exclude any series that is entirely empty or is missing data for the designated reference year.\n",
    "    * **Normalize Data**: Set 1985 as the reference year. Standardize the time series by dividing all values by the value in 1985, making the reference year's value equal to `1`.\n",
    "    * **Fit GAM Model**: Apply a Generalized Additive Model (using the `pyGAM` library) to fit a smoothed trend to the normalized time series.\n",
    "    * **Predict Trend**: Use the fitted GAM to predict the trend value for each year from 1950 to 2020.\n",
    "    * **Calculate Species-Level Index**: The index for the species is the series of predicted values divided by the predicted value of the reference year (1985).\n",
    "4.  **Aggregate to Global Level**:\n",
    "    * Combine the indices from all species by calculating the average value for each year.\n",
    "    * (Optional) Generate bootstrap confidence intervals (`low`, `high`) for the annual averages.\n",
    "5.  **Export Results**: Save the final aggregated data with the columns: `year`, `value`, `low`, `high`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流程（简化版 GAM 方法）（中文版）\n",
    "\n",
    "1.  **读取原始数据**：从源 CSV 文件中加载初始数据集。\n",
    "2.  **挑选数据列**：筛选数据，只保留 1950 年至 2020 年的年份列。\n",
    "3.  **处理每个物种**：对数据集中的每一个物种，执行以下步骤：\n",
    "    * **提取时间序列**：分离出该物种的时间序列数据。剔除掉完全为空或在参考年份（1985年）缺少数据的序列。\n",
    "    * **标准化数据**：将 1985 年设为参考年。通过将所有值除以 1985 年的值来进行标准化，使参考年的数值等于 `1`。\n",
    "    * **拟合 GAM 模型**：应用广义相加模型（使用 `pyGAM` 库）对标准化的时间序列进行平滑趋势拟合。\n",
    "    * **预测趋势**：使用拟合好的 GAM 模型预测 1950 年至 2020 年每一年的趋势值。\n",
    "    * **计算物种层面指数**：该物种的指数是预测值序列除以参考年（1985）的预测值。\n",
    "4.  **聚合到全局层面**：\n",
    "    * 通过计算每一年所有物种的平均值，来合并所有物种的指数。\n",
    "    * （可选）为年度平均值生成自举置信区间（`low`, `high`）。\n",
    "5.  **导出结果**：保存最终的聚合数据，包含以下列：`year`, `value`, `low`, `high`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Calculate Threatened Species Index using GAM and Bootstrapping\n",
    "\n",
    "This script calculates a composite macro-level index from a large number of individual species' time-series data. It employs a Generalized Additive Model (GAM) to smooth the trend for each species and uses bootstrapping to estimate the confidence interval of the overall trend.\n",
    "\n",
    "The workflow can be broken down into the following key steps:\n",
    "\n",
    "1.  **Initialization and Data Loading**: It starts by setting essential parameters, such as the reference year (`1985`), whether to apply a time lag, and bootstrap settings. It then loads the raw time-series data from the CSV file.\n",
    "2.  **Per-Species Processing**:\n",
    "    * **Normalization**: For each species' time series, the data is standardized against the value in the reference year, effectively setting the index for `1985 = 1`.\n",
    "    * **Smoothing and Fitting**: A GAM is fitted to the normalized data. This generates a smooth trend curve, which helps to fill in data gaps and reduce year-to-year noise.\n",
    "3.  **Aggregation and Statistical Analysis**:\n",
    "    * **Calculate Mean Trend**: All the individual smoothed curves are aggregated by taking the average across all species for each year. This results in the main index trend (`value`).\n",
    "    * **Estimate Uncertainty**: To quantify the confidence in the mean trend, a bootstrapping method is applied. It randomly resamples the species thousands of times (`n_boot=1000`), recalculates the mean trend for each sample, and uses the resulting distribution to determine the 95% confidence interval (`low` and `high` values).\n",
    "4.  **Export Results**: The final calculated columns (`year`, `value`, `low`, `high`) are saved to a new CSV file, ready for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指数表已保存（基准年=1985，滞后=True）：02_wrangled_data\\Table14_ThreatenedSpeciesIndexTable_version2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s\n",
    "\n",
    "# ====== 路径 ======\n",
    "input_path = r\"01_raw_data\\02_tsx-aggregated-data-dataset.csv\"\n",
    "output_path = r\"02_wrangled_data\\Table14_ThreatenedSpeciesIndexTable_version2.csv\"\n",
    "\n",
    "# ====== 参数 ======\n",
    "ref_year = 1985\n",
    "apply_lag = True          # 是否应用 3 年滞后\n",
    "lag_years = 3\n",
    "n_splines = 10\n",
    "n_boot = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ====== 读取数据 ======\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 年份列\n",
    "year_cols = [c for c in df.columns if c.isdigit()]\n",
    "years = np.array(list(map(int, year_cols)))\n",
    "if ref_year not in years:\n",
    "    raise ValueError(f\"参考年 {ref_year} 不在数据年份列中！\")\n",
    "ref_idx = list(years).index(ref_year)\n",
    "\n",
    "# ====== 对每条 time series 做GAM拟合（先/以1985标准化） ======\n",
    "species_curves = []\n",
    "for _, row in df.iterrows():\n",
    "    y = row[year_cols].values.astype(float)\n",
    "\n",
    "    # 参考年必须有效且 >0\n",
    "    if np.isnan(y[ref_idx]) or y[ref_idx] <= 0:\n",
    "        continue\n",
    "\n",
    "    # 相对化（以1985年为1）\n",
    "    y = y / y[ref_idx]\n",
    "\n",
    "    # 仅取有值的年份去拟合\n",
    "    mask = ~np.isnan(y)\n",
    "    x_obs = years[mask]\n",
    "    y_obs = y[mask]\n",
    "\n",
    "    # 至少需要3个点才能拟合\n",
    "    if len(x_obs) < 3:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        gam = LinearGAM(s(0, n_splines=n_splines)).fit(x_obs, y_obs)\n",
    "        y_pred = gam.predict(years)  # 预测全时段\n",
    "        species_curves.append(y_pred)\n",
    "    except Exception:\n",
    "        # 某些极端序列可能拟合失败，跳过即可\n",
    "        continue\n",
    "\n",
    "if len(species_curves) == 0:\n",
    "    raise RuntimeError(\"没有成功拟合的物种曲线，检查参考年或数据质量。\")\n",
    "\n",
    "species_curves = np.vstack(species_curves)  # 形状：[物种数, 年份数]\n",
    "\n",
    "# ====== 只保留 >= 1985 的年份；可选再做“末尾-3年”滞后 ======\n",
    "mask_years = years >= ref_year\n",
    "years_out = years[mask_years]\n",
    "curves = species_curves[:, mask_years]\n",
    "\n",
    "if apply_lag:\n",
    "    # 去掉末尾 lag_years 年\n",
    "    if len(years_out) > lag_years:\n",
    "        years_out = years_out[:-lag_years]\n",
    "        curves = curves[:, :-lag_years]\n",
    "\n",
    "# ====== 用bootstrap对“物种曲线集合”聚合，得到 value/low/high ======\n",
    "# 先计算“物种横截面均值曲线”\n",
    "mean_curve = np.nanmean(curves, axis=0)\n",
    "\n",
    "# 再通过对物种维度重采样，得到均值的不确定性\n",
    "boot_mat = np.empty((n_boot, curves.shape[1]), dtype=float)\n",
    "n_species = curves.shape[0]\n",
    "for b in range(n_boot):\n",
    "    idx = rng.integers(0, n_species, size=n_species)\n",
    "    sample = curves[idx, :]\n",
    "    boot_mat[b, :] = np.nanmean(sample, axis=0)\n",
    "\n",
    "low_ci = np.nanpercentile(boot_mat, 2.5, axis=0)\n",
    "high_ci = np.nanpercentile(boot_mat, 97.5, axis=0)\n",
    "\n",
    "# ====== 强制把参考年的三列设为1（对齐官方风格） ======\n",
    "# 注意此时years_out的第一个元素应该就是ref_year（因为我们截断了 <1985）\n",
    "if years_out[0] == ref_year:\n",
    "    mean_curve[0] = 1.0\n",
    "    low_ci[0] = 1.0\n",
    "    high_ci[0] = 1.0\n",
    "\n",
    "# ====== 导出 ======\n",
    "result = pd.DataFrame({\n",
    "    \"year\": years_out,\n",
    "    \"value\": mean_curve,\n",
    "    \"low\": low_ci,\n",
    "    \"high\": high_ci\n",
    "})\n",
    "result.to_csv(output_path, index=False)\n",
    "print(f\"指数表已保存（基准年={ref_year}，滞后={apply_lag}）：{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Create Schema for Threatened Species Index Table\n",
    "\n",
    "This step connects to the MySQL database and defines the structure for our new table, `Table14_ThreatenedSpeciesIndexTable`. The schema is designed to hold the time-series index data:\n",
    "- `year` is set as an `INT` and serves as the Primary Key.\n",
    "- `value`, `low`, and `high` are defined as `DOUBLE` to accommodate floating-point numbers.\n",
    "\n",
    "The `CREATE TABLE IF NOT EXISTS` command is used to ensure the script can be run multiple times without causing an error if the table already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL server, preparing to create Table14.\n",
      "Table 'Table14_ThreatenedSpeciesIndexTable' created successfully or already exists.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# --- Database connection configuration ---\n",
    "db_config = {\n",
    "    'host': 'database-plantx.cqz06uycysiz.us-east-1.rds.amazonaws.com',\n",
    "    'user': 'zihan',\n",
    "    'password': '2002317Yzh12138.',\n",
    "    'database': 'FIT5120_PlantX_Database',\n",
    "    'allow_local_infile': True,\n",
    "    'use_pure': True,\n",
    "    'charset': 'utf8mb4'\n",
    "}\n",
    "\n",
    "# --- Create the schema for Table14 ---\n",
    "try:\n",
    "    # Establish the database connection\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        print(\"Successfully connected to MySQL server, preparing to create Table14.\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Define the SQL query to create Table14\n",
    "        # `year` is set as an INT PRIMARY KEY\n",
    "        # `value`, `low`, and `high` are set as DOUBLE for floating-point numbers\n",
    "        create_table_14_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Table14_ThreatenedSpeciesIndexTable (\n",
    "            `year` INT PRIMARY KEY,\n",
    "            `value` DOUBLE,\n",
    "            `low` DOUBLE,\n",
    "            `high` DOUBLE\n",
    "        ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(create_table_14_query)\n",
    "        connection.commit()\n",
    "        print(\"Table 'Table14_ThreatenedSpeciesIndexTable' created successfully or already exists.\")\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error occurred while creating Table14: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Load CSV Data into Threatened Species Index Table\n",
    "\n",
    "After creating the table schema, this step populates it with data from the local CSV file (`02_wrangled_data/Table14_ThreatenedSpeciesIndexTable_version1.csv`).\n",
    "\n",
    "It uses the `LOAD DATA LOCAL INFILE` command, which is a highly efficient method for bulk-inserting data directly from a file into a MySQL table. The query is configured to handle the CSV format by specifying the field and line terminators and to skip the header row (`IGNORE 1 LINES`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL server, preparing to import data for Table14.\n",
      "Data import for Table14 successful! 37 rows affected.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# --- Import CSV data into Table14 ---\n",
    "try:\n",
    "    # Re-establish the database connection\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        print(\"Successfully connected to MySQL server, preparing to import data for Table14.\")\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Define the SQL query for loading data\n",
    "        # Using LOAD DATA LOCAL INFILE for efficient bulk import\n",
    "        load_data_query_14 = \"\"\"\n",
    "        LOAD DATA LOCAL INFILE '02_wrangled_data/Table14_ThreatenedSpeciesIndexTable_version1.csv'\n",
    "        INTO TABLE Table14_ThreatenedSpeciesIndexTable\n",
    "        CHARACTER SET utf8mb4\n",
    "        FIELDS TERMINATED BY ','\n",
    "        LINES TERMINATED BY '\\\\r\\\\n'\n",
    "        IGNORE 1 LINES\n",
    "        (year, value, low, high);\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(load_data_query_14)\n",
    "        connection.commit()\n",
    "        print(f\"Data import for Table14 successful! {cursor.rowcount} rows affected.\")\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error occurred during data import for Table14: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant Image Retrieval Query (CLIP ViT-B/32)\n",
    "\n",
    "Name: Zihan Yin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Imports and Path Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# ======== Path Config ========\n",
    "INDEX_PATH = Path(\"index/embeddings_fp16.npz\")   # vector file generated by indexing\n",
    "META_PATH  = Path(\"index/meta.json\")             # meta generated by indexing\n",
    "MODEL_DIR  = Path(\"clip-vit-b32\")                # local CLIP model directory\n",
    "MODEL_ID   = \"openai/clip-vit-base-patch32\"      # fallback if local not found\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Utilities (image preprocessing + TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exif_correct(img: Image.Image) -> Image.Image:\n",
    "    \"\"\"Fix orientation via EXIF and convert to RGB.\"\"\"\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    if img.mode == \"RGBA\":\n",
    "        bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "        return bg\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def tta_pipeline():\n",
    "    \"\"\"Light test-time augmentation (avoid heavy transforms).\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.06, 0.06), scale=(0.95, 1.05)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.01),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load Index and CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in index: 1319\n",
      "Embedding dimension: 512\n",
      "Failed to load local model; falling back to online.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load index\n",
    "npz = np.load(INDEX_PATH)\n",
    "db_emb = npz[\"embeddings\"].astype(np.float32)  # (N, D)\n",
    "plant_ids = npz[\"plant_ids\"]\n",
    "\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(\"Number of classes in index:\", db_emb.shape[0])\n",
    "print(\"Embedding dimension:\", db_emb.shape[1])\n",
    "\n",
    "# Load model (prefer on-line model)\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(MODEL_DIR)\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL_DIR)\n",
    "    print(\"Loaded local model successfully.\")\n",
    "except Exception:\n",
    "    print(\"Failed to load local model; falling back to online.\")\n",
    "    model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Encode Query Image (From User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_query(img: Image.Image, tta_num=8):\n",
    "    \"\"\"Encode the uploaded image with TTA and return a single vector.\"\"\"\n",
    "    img = exif_correct(img)\n",
    "    tta = tta_pipeline()\n",
    "    pil_list = [img] + [tta(img) for _ in range(max(tta_num-1, 0))]\n",
    "\n",
    "    inputs = processor(images=pil_list, return_tensors=\"pt\", do_center_crop=True, padding=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.autocast(device_type=(\"cuda\" if DEVICE==\"cuda\" else \"cpu\"),\n",
    "                        dtype=torch.float16, enabled=(DEVICE==\"cuda\")):\n",
    "        feats = model.get_image_features(**inputs)\n",
    "    feats = nn.functional.normalize(feats, p=2, dim=-1)\n",
    "    q = feats.mean(dim=0, keepdim=False)  # (D,)\n",
    "    return q.cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Compute Cosine Similarity and Take Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Top-K\n",
    "def topk_cosine(query_emb, db_emb, plant_ids, k=10):\n",
    "    \"\"\"\n",
    "    query_emb: (D,)\n",
    "    db_emb: (N, D) already L2-normalized\n",
    "    Return Top-K as (plant_id, score)\n",
    "    \"\"\"\n",
    "    sims = db_emb @ query_emb  # cosine similarity\n",
    "    idx = np.argpartition(-sims, kth=min(k, len(sims)-1))[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return [(int(plant_ids[i]), float(sims[i])) for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Example Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plant_id=342\tscore=0.8903\n",
      "plant_id=343\tscore=0.8866\n",
      "plant_id=341\tscore=0.8801\n",
      "plant_id=250\tscore=0.8756\n",
      "plant_id=540\tscore=0.8733\n",
      "plant_id=340\tscore=0.8713\n",
      "plant_id=402\tscore=0.8682\n",
      "plant_id=348\tscore=0.8681\n",
      "plant_id=742\tscore=0.8677\n",
      "plant_id=236\tscore=0.8664\n"
     ]
    }
   ],
   "source": [
    "# Allowed suffixes: .jpg, .webp, .JPG\n",
    "\n",
    "# Replace with the image path you want to query\n",
    "QUERY_IMG = Path(\"test_images/fdghjmhgferghthjkhgf.jpg\")  \n",
    "\n",
    "img = Image.open(QUERY_IMG)\n",
    "q_emb = encode_query(img, tta_num=8)\n",
    "\n",
    "results = topk_cosine(q_emb, db_emb, plant_ids, k=10)\n",
    "for pid, score in results:\n",
    "    print(f\"plant_id={pid}\\tscore={score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 植物检索建库（CLIP ViT-B/32）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — 安装依赖（已装可跳过）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 如果你的环境里已经有这些库，可以跳过本单元\n",
    "# # 注意：PyTorch 请按照你的 CUDA 版本安装（https://pytorch.org/），这里给出通用命令\n",
    "# %pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -q transformers pillow numpy tqdm safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — 导入与配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json, time, random, math\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# ======== 路径与参数 ========\n",
    "NOTEBOOK_DIR = Path(__file__).parent if \"__file__\" in globals() else Path().resolve()\n",
    "DATA_DIR = (NOTEBOOK_DIR / \"../../01_data_wrangling/01_raw_data/05_thumbnail_image\").resolve()\n",
    "OUT_DIR    = Path(\"index\")                            # 输出 embeddings 与 meta\n",
    "MODEL_ID   = \"openai/clip-vit-base-patch32\"          # CLIP ViT-B/32\n",
    "MODEL_DIR  = Path(\"clip-vit-b32\")             # 离线保存模型\n",
    "NUM_AUGS   = 20                                      # 每类增强次数（不含原图）\n",
    "BATCH_SIZE = 32                                      # 编码 batch\n",
    "SEED       = 42\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 文件名正则：从文件名里抓 plant_id\n",
    "FILE_RE = re.compile(r\"plant_species_thumbnail_image_(\\d+)\\.jpg\", re.IGNORECASE)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录存在？ True\n",
      "找到的jpg数： 541\n",
      "前5个文件： ['plant_species_thumbnail_image_1.jpg', 'plant_species_thumbnail_image_10.jpg', 'plant_species_thumbnail_image_100.jpg', 'plant_species_thumbnail_image_101.jpg', 'plant_species_thumbnail_image_102.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"目录存在？\", DATA_DIR.exists())\n",
    "print(\"找到的jpg数：\", len(list(DATA_DIR.glob(\"*.jpg\"))))\n",
    "print(\"前5个文件：\", [p.name for p in DATA_DIR.glob(\"*.jpg\")][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — 工具函数（EXIF 修正、轻量增强、文件扫描）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 541 个类别（每类 1 张缩略图）\n"
     ]
    }
   ],
   "source": [
    "def exif_correct(img: Image.Image) -> Image.Image:\n",
    "    # 统一方向；将带 alpha 的 PNG 转为 RGB（白底）\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    if img.mode == \"RGBA\":\n",
    "        bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "        return bg\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def light_aug_pipeline():\n",
    "    \"\"\"\n",
    "    轻量增强（避免“过猛”导致原型漂移）：\n",
    "    - 随机水平翻转\n",
    "    - 亮度/对比/饱和度/色相 小幅抖动\n",
    "    - 轻旋转与平移，适度缩放\n",
    "    - 低概率轻模糊\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.10, 0.10), scale=(0.9, 1.1)),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.2),\n",
    "    ])\n",
    "\n",
    "def parse_plant_images(data_dir: Path):\n",
    "    pairs = []\n",
    "    for p in sorted(data_dir.glob(\"*\")):\n",
    "        m = FILE_RE.match(p.name)\n",
    "        if m:\n",
    "            plant_id = int(m.group(1))\n",
    "            pairs.append((plant_id, p))\n",
    "    return pairs\n",
    "\n",
    "pairs = parse_plant_images(DATA_DIR)\n",
    "print(f\"发现 {len(pairs)} 个类别（每类 1 张缩略图）\")\n",
    "assert len(pairs) > 0, \"未发现符合命名规则的图片，请检查目录与文件名格式。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — 加载 CLIP 模型与处理器（并离线保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 CLIP 模型与处理器…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zyyin1\\anaconda3\\envs\\python_for_ai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyyin1\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"加载 CLIP 模型与处理器…\")\n",
    "model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval().to(device)\n",
    "\n",
    "# 保存一份到本地，便于离线/部署\n",
    "# model.save_pretrained(MODEL_DIR)\n",
    "processor.save_pretrained(MODEL_DIR)\n",
    "\n",
    "embedding_dim = model.config.projection_dim  # 512\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — 批量编码函数（GPU 混合精度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(pil_list, batch_size=32, amp_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    使用 CLIPProcessor 保持与官方一致的预处理（resize/center crop/normalize 等）\n",
    "    输出为 L2 归一化后的图像 embedding（B, D）\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    for i in range(0, len(pil_list), batch_size):\n",
    "        batch = pil_list[i:i+batch_size]\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\", do_center_crop=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.autocast(device_type=(\"cuda\" if device==\"cuda\" else \"cpu\"),\n",
    "                            dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
    "            feats = model.get_image_features(**inputs)  # (B, D)\n",
    "        feats = nn.functional.normalize(feats, p=2, dim=-1)\n",
    "        embs.append(feats.cpu())\n",
    "    return torch.cat(embs, dim=0)  # (N, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — 构建均值原型并缓存（主循环）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "编码各类别（含增强）: 100%|██████████| 541/541 [04:40<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings 形状： (541, 512)\n",
      "示例 L2 范数（应接近 1）： 0.99983567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aug = light_aug_pipeline()\n",
    "\n",
    "# 预分配：可能有坏图跳过，先用 list 收集更稳妥\n",
    "proto_list = []\n",
    "plant_id_list = []\n",
    "\n",
    "pbar = tqdm(pairs, desc=\"编码各类别（含增强）\")\n",
    "\n",
    "for plant_id, img_path in pbar:\n",
    "    # 打开 & 纠正\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 跳过损坏图片 {img_path}: {e}\")\n",
    "        continue\n",
    "    img = exif_correct(img)\n",
    "\n",
    "    # 生成增强样本（含原图）\n",
    "    pil_list = [img] + [aug(img) for _ in range(NUM_AUGS)]\n",
    "\n",
    "    # 编码\n",
    "    feats = encode_images(pil_list, batch_size=BATCH_SIZE)  # (N, D)\n",
    "    mean_proto = feats.mean(dim=0, keepdim=True)\n",
    "    mean_proto = nn.functional.normalize(mean_proto, p=2, dim=-1).cpu().numpy()[0]  # (D,)\n",
    "\n",
    "    # 缓存\n",
    "    proto_list.append(mean_proto.astype(np.float32))\n",
    "    plant_id_list.append(plant_id)\n",
    "\n",
    "# 组装矩阵\n",
    "embeddings = np.vstack(proto_list).astype(np.float32)  # (C, D)\n",
    "plant_ids  = np.array(plant_id_list, dtype=np.int64)\n",
    "\n",
    "print(\"embeddings 形状：\", embeddings.shape)\n",
    "print(\"示例 L2 范数（应接近 1）：\", np.linalg.norm(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — 保存索引（fp16 压缩）与 Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已保存： embeddings_fp16.npz\n",
      "✅ 已保存： meta.json\n",
      "✅ 模型保存在： clip-vit-b32\n"
     ]
    }
   ],
   "source": [
    "# 使用 fp16 存盘（节省空间），查询时会转回 fp32 计算\n",
    "emb_fp16 = embeddings.astype(np.float16)\n",
    "np.savez_compressed(OUT_DIR / \"embeddings_fp16.npz\",\n",
    "                    embeddings=emb_fp16,\n",
    "                    plant_ids=plant_ids)\n",
    "\n",
    "meta = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"model_local_dir\": str(MODEL_DIR.as_posix()),\n",
    "    \"embedding_dim\": int(embedding_dim),\n",
    "    \"num_classes\": int(embeddings.shape[0]),\n",
    "    \"num_augs_per_class\": NUM_AUGS,\n",
    "    \"built_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device_used\": device,\n",
    "    \"seed\": SEED,\n",
    "    \"preprocess\": {\n",
    "        \"center_crop\": True,\n",
    "        \"note\": \"与 CLIPProcessor 默认预处理保持一致；在线查询需完全一致\"\n",
    "    },\n",
    "    \"similarity\": \"cosine (via dot on L2-normalized vectors)\"\n",
    "}\n",
    "with open(OUT_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 已保存：\", (OUT_DIR / \"embeddings_fp16.npz\").as_posix())\n",
    "print(\"✅ 已保存：\", (OUT_DIR / \"meta.json\").as_posix())\n",
    "print(\"✅ 模型保存在：\", MODEL_DIR.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — 快速自检（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入 embeddings： (541, 512)  载入 plant_ids： (541,)\n",
      "平均范数： 0.999998\n",
      "展示前 5 个 plant_id： [  1  10 100 101 102]\n"
     ]
    }
   ],
   "source": [
    "# 加载刚保存的索引，做一次形状与范数检查\n",
    "npz = np.load(OUT_DIR / \"embeddings_fp16.npz\")\n",
    "E = npz[\"embeddings\"].astype(np.float32)  # (C, D)\n",
    "P = npz[\"plant_ids\"]\n",
    "\n",
    "print(\"载入 embeddings：\", E.shape, \" 载入 plant_ids：\", P.shape)\n",
    "print(\"平均范数：\", np.linalg.norm(E, axis=1).mean())\n",
    "print(\"展示前 5 个 plant_id：\", P[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

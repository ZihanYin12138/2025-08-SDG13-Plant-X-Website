{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant Image Retrieval Indexing (CLIP ViT-B/32)\n",
    "\n",
    "Name: Zihan Yin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install -q transformers pillow numpy tqdm safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json, time, random, math\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# ======== Paths & Params ========\n",
    "NOTEBOOK_DIR = Path(__file__).parent if \"__file__\" in globals() else Path().resolve()\n",
    "DATA_DIR = (NOTEBOOK_DIR / \"../../01_data_wrangling/01_raw_data/05_thumbnail_image\").resolve()\n",
    "OUT_DIR    = Path(\"index\")                            # output embeddings & meta\n",
    "MODEL_ID   = \"openai/clip-vit-base-patch32\"          # CLIP ViT-B/32\n",
    "MODEL_DIR  = Path(\"clip-vit-b32\")                    # local model cache\n",
    "NUM_AUGS   = 20                                      # augmentations per class (excluding original)\n",
    "BATCH_SIZE = 32                                      # encoding batch size\n",
    "SEED       = 42\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Filename regex: capture plant_id from filename\n",
    "FILE_RE = re.compile(r\"plant_species_thumbnail_image_(\\d+)\\.jpg\", re.IGNORECASE)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Dataset Peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists? True\n",
      "Number of JPGs found: 1319\n",
      "First 5 files: ['plant_species_thumbnail_image_1.jpg', 'plant_species_thumbnail_image_10.jpg', 'plant_species_thumbnail_image_100.jpg', 'plant_species_thumbnail_image_1000.jpg', 'plant_species_thumbnail_image_1001.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"Directory exists?\", DATA_DIR.exists())\n",
    "print(\"Number of JPGs found:\", len(list(DATA_DIR.glob(\"*.jpg\"))))\n",
    "print(\"First 5 files:\", [p.name for p in DATA_DIR.glob(\"*.jpg\")][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Utils Tools (EXIF fix, light augment, file scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1319 classes (one thumbnail each)\n"
     ]
    }
   ],
   "source": [
    "def exif_correct(img: Image.Image) -> Image.Image:\n",
    "    # Normalize orientation; convert RGBA to RGB (white background)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    if img.mode == \"RGBA\":\n",
    "        bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "        return bg\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def light_aug_pipeline():\n",
    "    \"\"\"\n",
    "    Light augmentations to avoid prototype drift:\n",
    "    - Random horizontal flip\n",
    "    - Mild brightness/contrast/saturation/hue jitter\n",
    "    - Small rotation/translation + moderate scale jitter\n",
    "    - Low-probability mild blur\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.02),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.10, 0.10), scale=(0.9, 1.1)),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.2),\n",
    "    ])\n",
    "\n",
    "def parse_plant_images(data_dir: Path):\n",
    "    pairs = []\n",
    "    for p in sorted(data_dir.glob(\"*\")):\n",
    "        m = FILE_RE.match(p.name)\n",
    "        if m:\n",
    "            plant_id = int(m.group(1))\n",
    "            pairs.append((plant_id, p))\n",
    "    return pairs\n",
    "\n",
    "pairs = parse_plant_images(DATA_DIR)\n",
    "print(f\"Found {len(pairs)} classes (one thumbnail each)\")\n",
    "assert len(pairs) > 0, \"No images found matching the naming pattern; check directory and filenames.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Load CLIP Model and Processor (cache locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model and processor…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading CLIP model and processor…\")\n",
    "model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval().to(device)\n",
    "\n",
    "# Optionally cache locally for offline/deployment\n",
    "# model.save_pretrained(MODEL_DIR) ###############\n",
    "processor.save_pretrained(MODEL_DIR)\n",
    "\n",
    "embedding_dim = model.config.projection_dim  # 512\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Batch Encoding (AMP on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_images(pil_list, batch_size=32, amp_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Use CLIPProcessor for canonical preprocessing (resize/center crop/normalize).\n",
    "    Return L2-normalized image embeddings (B, D).\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    for i in range(0, len(pil_list), batch_size):\n",
    "        batch = pil_list[i:i+batch_size]\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\", do_center_crop=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.autocast(device_type=(\"cuda\" if device==\"cuda\" else \"cpu\"),\n",
    "                            dtype=amp_dtype, enabled=(device==\"cuda\")):\n",
    "            feats = model.get_image_features(**inputs)  # (B, D)\n",
    "        feats = nn.functional.normalize(feats, p=2, dim=-1)\n",
    "        embs.append(feats.cpu())\n",
    "    return torch.cat(embs, dim=0)  # (N, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Build Mean Prototypes and Cache (main loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding classes (with augmentation): 100%|██████████| 1319/1319 [11:35<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1319, 512)\n",
      "Example L2 norm (≈1): 0.99983567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aug = light_aug_pipeline()\n",
    "\n",
    "# Lists for robust collection (skip corrupted images)\n",
    "proto_list = []\n",
    "plant_id_list = []\n",
    "\n",
    "pbar = tqdm(pairs, desc=\"Encoding classes (with augmentation)\")\n",
    "\n",
    "for plant_id, img_path in pbar:\n",
    "    # Open & correct\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skip corrupted image {img_path}: {e}\")\n",
    "        continue\n",
    "    img = exif_correct(img)\n",
    "\n",
    "    # Build augmented samples (include original)\n",
    "    pil_list = [img] + [aug(img) for _ in range(NUM_AUGS)]\n",
    "\n",
    "    # Encode\n",
    "    feats = encode_images(pil_list, batch_size=BATCH_SIZE)  # (N, D)\n",
    "    mean_proto = feats.mean(dim=0, keepdim=True)\n",
    "    mean_proto = nn.functional.normalize(mean_proto, p=2, dim=-1).cpu().numpy()[0]  # (D,)\n",
    "\n",
    "    # Cache\n",
    "    proto_list.append(mean_proto.astype(np.float32))\n",
    "    plant_id_list.append(plant_id)\n",
    "\n",
    "# Assemble matrices\n",
    "embeddings = np.vstack(proto_list).astype(np.float32)  # (C, D)\n",
    "plant_ids  = np.array(plant_id_list, dtype=np.int64)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Example L2 norm (≈1):\", np.linalg.norm(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Save Index (fp16) and Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: index/embeddings_fp16.npz\n",
      "Saved: index/meta.json\n",
      "Model cached at: clip-vit-b32\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Persist Artifacts\n",
    "# Store embeddings in fp16 (space-efficient); queries can upcast to fp32\n",
    "emb_fp16 = embeddings.astype(np.float16)\n",
    "np.savez_compressed(OUT_DIR / \"embeddings_fp16.npz\",\n",
    "                    embeddings=emb_fp16,\n",
    "                    plant_ids=plant_ids)\n",
    "\n",
    "meta = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"model_local_dir\": str(MODEL_DIR.as_posix()),\n",
    "    \"embedding_dim\": int(embedding_dim),\n",
    "    \"num_classes\": int(embeddings.shape[0]),\n",
    "    \"num_augs_per_class\": NUM_AUGS,\n",
    "    \"built_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device_used\": device,\n",
    "    \"seed\": SEED,\n",
    "    \"preprocess\": {\n",
    "        \"center_crop\": True,\n",
    "        \"note\": \"Aligned with CLIPProcessor defaults; online queries must match\"\n",
    "    },\n",
    "    \"similarity\": \"cosine (via dot on L2-normalized vectors)\"\n",
    "}\n",
    "with open(OUT_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", (OUT_DIR / \"embeddings_fp16.npz\").as_posix())\n",
    "print(\"Saved:\", (OUT_DIR / \"meta.json\").as_posix())\n",
    "print(\"Model cached at:\", MODEL_DIR.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Reload and Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (1319, 512)  Loaded plant_ids: (1319,)\n",
      "Mean L2 norm: 1.0000056\n",
      "First 5 plant_ids: [   1   10  100 1000 1001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the saved index and check shapes/norms\n",
    "npz = np.load(OUT_DIR / \"embeddings_fp16.npz\")\n",
    "E = npz[\"embeddings\"].astype(np.float32)  # (C, D)\n",
    "P = npz[\"plant_ids\"]\n",
    "\n",
    "print(\"Loaded embeddings:\", E.shape, \" Loaded plant_ids:\", P.shape)\n",
    "print(\"Mean L2 norm:\", np.linalg.norm(E, axis=1).mean())\n",
    "print(\"First 5 plant_ids:\", P[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
